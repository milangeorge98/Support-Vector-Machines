{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCa7seky2YDr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1>Assignment 4</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "198ZoDEZ3FtN"
   },
   "source": [
    "# 1. <font color='#556b2f'> **Support Vector Machines with Synthetic Data**</font>, 50 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewqefiUP3p7a"
   },
   "source": [
    "For this problem, we will generate synthetic data for a nonlinear binary classification problem and partition it into training, validation and test sets. Our goal is to understand the behavior of SVMs with Radial-Basis Function (RBF) kernels with different values of $C$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhGIqSvX1_Sl"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2651250381.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/11/vg0z1pm51vq6zyphp1g0q6k40000gn/T/ipykernel_12098/2651250381.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    m = 30\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS FUNCTION; IF YOU WANT TO PLAY AROUND WITH DATA GENERATION, \n",
    "# MAKE A COPY OF THIS FUNCTION AND THEN EDIT\n",
    "#\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def generate_data(n_samples, tst_frac=0.2, val_frac=0.2):\n",
    "  # Generate a non-linear data set\n",
    "  X, y = make_moons(n_samples=n_samples, noise=0.25, random_state=42)\n",
    "   \n",
    "  # Take a small subset of the data and make it VERY noisy; that is, generate outliers\n",
    "    m = 30\n",
    "    np.random.seed(30)  # Deliberately use a different seed\n",
    "    ind = np.random.permutation(n_samples)[:m]\n",
    "    X[ind, :] += np.random.multivariate_normal([0, 0], np.eye(2), (m, ))\n",
    "    y[ind] = 1 - y[ind]\n",
    "\n",
    "    # Plot this data\n",
    "    cmap = ListedColormap(['#b30065', '#178000'])  \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolors='k')       \n",
    "\n",
    "    # First, we use train_test_split to partition (X, y) into training and test sets\n",
    "    X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=tst_frac, \n",
    "                                                random_state=42)\n",
    "\n",
    "    # Next, we use train_test_split to further partition (X_trn, y_trn) into training and validation sets\n",
    "    X_trn, X_val, y_trn, y_val = train_test_split(X_trn, y_trn, test_size=val_frac, \n",
    "                                                random_state=42)\n",
    "\n",
    "    return (X_trn, y_trn), (X_val, y_val), (X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1CeM8EK4QHj"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  DO NOT EDIT THIS FUNCTION; IF YOU WANT TO PLAY AROUND WITH VISUALIZATION, \n",
    "#  MAKE A COPY OF THIS FUNCTION AND THEN EDIT \n",
    "#\n",
    "\n",
    "def visualize(models, param, X, y):\n",
    "  # Initialize plotting\n",
    "    if len(models) % 3 == 0:\n",
    "    nrows = len(models) // 3\n",
    "    else:\n",
    "    nrows = len(models) // 3 + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=3, figsize=(15, 5.0 * nrows))\n",
    "    cmap = ListedColormap(['#b30065', '#178000'])\n",
    "\n",
    "    # Create a mesh\n",
    "    xMin, xMax = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    yMin, yMax = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xMesh, yMesh = np.meshgrid(np.arange(xMin, xMax, 0.01), \n",
    "                             np.arange(yMin, yMax, 0.01))\n",
    "\n",
    "    for i, (p, clf) in enumerate(models.items()):\n",
    "    # if i > 0:\n",
    "    #   break\n",
    "    r, c = np.divmod(i, 3)\n",
    "    ax = axes[r, c]\n",
    "\n",
    "    # Plot contours\n",
    "    zMesh = clf.decision_function(np.c_[xMesh.ravel(), yMesh.ravel()])\n",
    "    zMesh = zMesh.reshape(xMesh.shape)\n",
    "    ax.contourf(xMesh, yMesh, zMesh, cmap=plt.cm.PiYG, alpha=0.6)\n",
    "\n",
    "    if (param == 'C' and p > 0.0) or (param == 'gamma'):\n",
    "        ax.contour(xMesh, yMesh, zMesh, colors='k', levels=[-1, 0, 1], \n",
    "                 alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Plot data\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolors='k')       \n",
    "    ax.set_title('{0} = {1}'.format(param, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2vksl6V4YQ1"
   },
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "n_samples = 300    # Total size of data set \n",
    "(X_trn, y_trn), (X_val, y_val), (X_tst, y_tst) = generate_data(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0stHFSe4fnH"
   },
   "source": [
    "---\n",
    "### **a**. (25 points)  The effect of the regularization parameter, $C$\n",
    "Complete the Python code snippet below that takes the generated synthetic 2-d data as input and learns non-linear SVMs. Use scikit-learn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) function to learn SVM models with **radial-basis kernels** for fixed $\\gamma$ and various choices of $C \\in \\{10^{-3}, 10^{-2}\\, \\cdots, 1, \\, \\cdots\\, 10^5\\}$. The value of $\\gamma$ is fixed to $\\gamma = \\frac{1}{d \\cdot \\sigma_X}$, where $d$ is the data dimension and $\\sigma_X$ is the standard deviation of the data set $X$. SVC can automatically use these setting for $\\gamma$ if you pass the argument gamma = 'scale' (see documentation for more details).\n",
    "\n",
    "**Plot**: For each classifier, compute **both** the **training error** and the **validation error**. Plot them together, making sure to label the axes and each curve clearly.\n",
    "\n",
    "**Discussion**: How do the training error and the validation error change with $C$? Based on the visualization of the models and their resulting classifiers, how does changing $C$ change the models? Explain in terms of minimizing the SVM's objective function $\\frac{1}{2} \\mathbf{w}^\\prime \\mathbf{w} \\, + \\, C \\, \\Sigma_{i=1}^n \\, \\ell(\\mathbf{w} \\mid \\mathbf{x}_i, y_i)$, where $\\ell$ is the hinge loss for each training example $(\\mathbf{x}_i, y_i)$.\n",
    "\n",
    "**Final Model Selection**: Use the validation set to select the best the classifier corresponding to the best value, $C_{best}$. Report the accuracy on the **test set** for this selected best SVM model. _Note: You should report a single number, your final test set accuracy on the model corresponding to $C_{best}$_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t63YP-bJ4hzJ"
   },
   "outputs": [],
   "source": [
    "# Learn support vector classifiers with a radial-basis function kernel with \n",
    "# fixed gamma = 1 / (n_features * X.std()) and different values of C\n",
    "C_range = np.arange(-3.0, 6.0, 1.0)\n",
    "C_values = np.power(10.0, C_range)\n",
    "\n",
    "models = dict()\n",
    "trnErr = dict()\n",
    "valErr = dict()\n",
    "target_class = None\n",
    "\n",
    "minimum = 1\n",
    "for C in C_values:\n",
    "\n",
    "    model = SVC(C=C , kernel = \"rbf\", gamma = \"scale\",random_state=0)\n",
    "    model.fit(X_trn, y_trn)\n",
    "    models[C]=model\n",
    "    y_trainpred = model.predict(X_trn)\n",
    "    y_valpred = model.predict(X_val)\n",
    "    trnErr[C]=1-accuracy_score(y_trn,y_trainpred)\n",
    "    valErr[C]=1-accuracy_score(y_val,y_valpred)\n",
    "    if(valErr[C] < minimum):\n",
    "        minimum=valErr[C]\n",
    "        target_class=C\n",
    "   \n",
    "  \n",
    "visualize(models, 'C', X_trn, y_trn)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.log10(list(valErr.keys())), list(valErr.values()), marker='X', linewidth=3, markersize=12)\n",
    "plt.plot(np.log10(list(trnErr.keys())), list(trnErr.values()), marker='D', linewidth=3, markersize=12)\n",
    "plt.xlabel('10^C', fontsize=18)\n",
    "plt.ylabel('Validation/Train error', fontsize=18)\n",
    "plt.xticks(np.log10(list(valErr.keys())), fontsize=12)\n",
    "plt.legend(['Validation Error', 'Training Error'], fontsize=18)\n",
    "\n",
    "predict = models[target_class].predict(X_tst)\n",
    "score = metrics.accuracy_score(y_tst, predict)\n",
    "print(\"Accuracy: {0:.2f}%, for C = {1:1.0f}\".format(score * 100,target_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing values of 'C', we can observe that the training error decreases but after C=100, the validation error increases and decreases intermittently. Larger values of 'C' yields a smaller margin whereas a smaller value of 'C' yields a wider margin. The increase in validation error is due to overfitting. Large values of C makes missclassification stricter hence, the smaller margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C = 100,gamma='scale',kernel='rbf')\n",
    "model.fit(X_trn, y_trn)\n",
    "y_test_pred=model.predict(X_tst)\n",
    "test_error=1-accuracy_score(y_tst,y_test_pred)\n",
    "print('Test Error  : '+ str(test_error))\n",
    "print(accuracy_score(y_tst,y_test_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evEeXgSE9Hss"
   },
   "source": [
    "---\n",
    "### **b**. (25 points)  The effect of the RBF kernel parameter, $\\gamma$\n",
    "Complete the Python code snippet below that takes the generated synthetic 2-d data as input and learns various non-linear SVMs. Use scikit-learn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) function to learn SVM models with **radial-basis kernels** for fixed $C$ and various choices of $\\gamma \\in \\{10^{-2}, 10^{-1}\\, 1, 10, \\, 10^2 \\, 10^3\\}$. The value of $C$ is fixed to $C = 10$.\n",
    "\n",
    "**Plot**: For each classifier, compute **both** the **training error** and the **validation error**. Plot them together, making sure to label the axes and each curve clearly.\n",
    "\n",
    "**Discussion**: How do the training error and the validation error change with $\\gamma$? Based on the visualization of the models and their resulting classifiers, how does changing $\\gamma$ change the models? Explain in terms of the functional form of the RBF kernel, $\\kappa(\\mathbf{x}, \\,\\mathbf{z}) \\, = \\, \\exp(-\\gamma \\cdot \\|\\mathbf{x} - \\mathbf{z} \\|^2)$\n",
    "\n",
    "**Final Model Selection**: Use the validation set to select the best the classifier corresponding to the best value, $\\gamma_{best}$. Report the accuracy on the **test set** for this selected best SVM model. _Note: You should report a single number, your final test set accuracy on the model corresponding to $\\gamma_{best}$_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYzLo_zv-bIk"
   },
   "outputs": [],
   "source": [
    "# Learn support vector classifiers with a radial-basis function kernel with \n",
    "# fixed C = 10.0 and different values of gamma\n",
    "gamma_range = np.arange(-2.0, 4.0, 1.0)\n",
    "gamma_values = np.power(10.0, gamma_range)\n",
    "\n",
    "models = dict()\n",
    "trnErr = dict()\n",
    "valErr = dict()\n",
    "target_g = None\n",
    "minimum=1\n",
    "\n",
    "for G in gamma_values:\n",
    "    model = SVC(C=10.0, kernel = \"rbf\", gamma = G)\n",
    "    models[G] = model.fit(X_trn, y_trn)\n",
    "    trnErr[G] = 1-models[G].score(X_trn, y_trn)\n",
    "    valErr[G] = 1-models[G].score(X_val, y_val)\n",
    "    if valErr[G] < minimum:\n",
    "        minimum = valErr[G]\n",
    "        target_g = G\n",
    "  \n",
    "visualize(models, 'gamma', X_trn, y_trn)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.log10(list(valErr.keys())), list(valErr.values()), marker='o', linewidth=4, markersize=12)\n",
    "plt.plot(np.log10(list(trnErr.keys())), list(trnErr.values()), marker='s', linewidth=4, markersize=12)\n",
    "plt.xlabel('Gamma= 10^n', fontsize=18)\n",
    "plt.ylabel('Validation/Train error', fontsize=18)\n",
    "plt.xticks(np.log10(list(valErr.keys())), fontsize=12)\n",
    "plt.legend(['Validation Error', 'Training Error'], fontsize=18)\n",
    "\n",
    "predict = models[target_g].predict(X_tst)\n",
    "score = metrics.accuracy_score(y_tst, predict)\n",
    "print(\"Accuracy: {0:.2f}%, for ð›¾ = {1:1.0f}\".format(score * 100,target_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error decreases for increasing values of gamma however, the validation error decreases for smaller values of gamma and correspondingly increases for higher values of gamma indicating and overfitting of the model. \n",
    "Low gamma values imply broader decision regions and higher gamma values imply closer influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C = 10,gamma=1.0,kernel='rbf')\n",
    "model.fit(X_trn, y_trn)\n",
    "y_test_pred = model.predict(X_tst)\n",
    "testErr = accuracy_score(y_tst,y_test_pred)*100\n",
    "print('Test Error was evaluated to be : '+ str(testErr)+' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7K44mi5_3LrL"
   },
   "source": [
    "---\n",
    "# 2. <font color='#556b2f'> **Breast Cancer Diagnosis with Support Vector Machines**</font>, 25 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAwcBYkQ_KdK"
   },
   "source": [
    "For this problem, we will use the [Wisconsin Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) data set, which has already been pre-processed and partitioned into training, validation and test sets. Numpy's [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) command can be used to load CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zrb6AvCJ_WYX"
   },
   "outputs": [],
   "source": [
    "# Load the Breast Cancer Diagnosis data set; download the files from eLearning\n",
    "# CSV files can be read easily using np.loadtxt()\n",
    "#\n",
    "# Insert your code here.\n",
    "#\n",
    "\n",
    "X_trn=np.loadtxt('wdbc_trn.csv', delimiter = ',')\n",
    "X_test=np.loadtxt('wdbc_tst.csv', delimiter = ',')\n",
    "X_val=np.loadtxt('wdbc_val.csv', delimiter=',')\n",
    "\n",
    "XTrain=X_trn[:, 1:]\n",
    "XTest=X_test[:, 1:]\n",
    "XVal=X_val[:, 1:]\n",
    "YTrain=X_trn[:,0]\n",
    "YTest=X_test[:,0]\n",
    "YVal=X_val[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0VvyMt3AAZQ"
   },
   "source": [
    "Use scikit-learn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) function to learn SVM models with **radial-basis kernels** for **each combination** of $C \\in \\{10^{-2}, 10^{-1}, 1, 10^1, \\, \\cdots\\, 10^4\\}$ and $\\gamma \\in \\{10^{-3}, 10^{-2}\\, 10^{-1}, 1, \\, 10, \\, 10^2\\}$. Print the tables corresponding to the training and validation errors.\n",
    "\n",
    "**Final Model Selection**: Use the validation set to select the best the classifier corresponding to the best parameter values, $C_{best}$ and $\\gamma_{best}$. Report the accuracy on the **test set** for this selected best SVM model. _Note: You should report a single number, your final test set accuracy on the model corresponding to $C_{best}$ and $\\gamma_{best}$_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPjbglVuBYt3"
   },
   "outputs": [],
   "source": [
    "C_range      = np.arange(-2.0, 5.0, 1.0)\n",
    "C_values     = np.power(10.0, C_range)\n",
    "gamma_range  = np.arange(-3.0, 3.0, 1.0)\n",
    "gamma_values = np.power(10.0, gamma_range)\n",
    "\n",
    "models             = dict()\n",
    "trainError         = dict()\n",
    "validationError    = dict()\n",
    "target_C, target_G = None,None\n",
    "\n",
    "minimum=1\n",
    "for C in C_values:\n",
    "    for G in gamma_values:\n",
    "        model = SVC(C=C, kernel = \"rbf\", gamma = G)\n",
    "        models[(C,G)]=model.fit(XTrain, YTrain)\n",
    "        y_pred=model.predict(XVal)\n",
    "        trainError[(C,G)] = (1- model.score(XTrain, YTrain))\n",
    "        validationError[(C,G)] = (1- metrics.accuracy_score(YVal, y_pred))\n",
    "        if validationError[(C,G)] < minimum:\n",
    "            minimum = validationError[(C, G)]\n",
    "            target_C = C\n",
    "            target_G = G\n",
    "            \n",
    "predicted = models[target_C, target_G].predict(XTest)\n",
    "accuracy = metrics.accuracy_score(YTest, predicted)\n",
    "print(\"Accuracy: {0:.2f}%, for G = {1:1.3f} and C = {2:1.0f}\".format(accuracy * 100,target_G,target_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <font color='#556b2f'> **Breast Cancer Diagnosis with $k$-Nearest Neighbors**</font>, 25 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's [k-nearest neighbor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classifier to learn models for Breast Cancer Diagnosis with $k \\in \\{1, \\, 5, \\, 11, \\, 15, \\, 21\\}$, with the kd-tree algorithm.\n",
    "\n",
    "**Plot**: For each classifier, compute **both** the **training error** and the **validation error**. Plot them together, making sure to label the axes and each curve clearly.\n",
    "\n",
    "**Final Model Selection**: Use the validation set to select the best the classifier corresponding to the best parameter value, $k_{best}$. Report the accuracy on the **test set** for this selected best kNN model. _Note: You should report a single number, your final test set accuracy on the model corresponding to $k_{best}$_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model selection\n",
    "k=[1, 5, 11, 15, 21]\n",
    "\n",
    "models          = dict()\n",
    "trainError      = dict()\n",
    "validationError = dict()\n",
    "minimum         = 1\n",
    "target_n        = None\n",
    "\n",
    "for n in k:\n",
    "    model = KNeighborsClassifier(n_neighbors= n, algorithm = 'kd_tree')\n",
    "    models[n] = model.fit(XTrain, YTrain)\n",
    "    y_pred = model.predict(XVal)\n",
    "    trainError[n] = (1- models[n].score(XTrain, YTrain))\n",
    "    validationError[n] = (1- metrics.accuracy_score(YVal, y_pred))\n",
    "    if validationError[n] < minimum:\n",
    "        minimum = validationError[n]\n",
    "        target_n = n\n",
    "        \n",
    "predicted = models[target_n].predict(XTest)\n",
    "accuracy = metrics.accuracy_score(YTest, predicted)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(validationError.keys()), list(validationError.values()), marker='o', linewidth=4, markersize=12)\n",
    "plt.plot(list(trainError.keys()), list(trainError.values()), marker='s', linewidth=4, markersize=12)\n",
    "plt.xlabel('No of neighbors', fontsize=18)\n",
    "plt.ylabel('Validation/Train error', fontsize=18)\n",
    "plt.xticks(list(validationError.keys()), fontsize=12)\n",
    "plt.legend(['Validation Error', 'Training Error'], fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that k=5 and k=11 yield the least validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor = KNeighborsClassifier(n_neighbors=11,algorithm='kd_tree')\n",
    "neighbor.fit(XTrain,YTrain) \n",
    "y_test_pred=neighbor.predict(XTest)\n",
    "print('Test Accuracy')\n",
    "print(accuracy_score(YTest,y_test_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: Which of these two approaches, SVMs or kNN, would you prefer for this classification task? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that KNN yields a better accuracy (97.39%) as compared to SVM (95.65%). KNN works better with more number of attributes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS6301-Homework2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
